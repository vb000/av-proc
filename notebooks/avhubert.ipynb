{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import git\n",
    "\n",
    "FFMPEG = '/mmfs1/gscratch/cse/bandhav/miniconda3/envs/avhubert/bin/ffmpeg'\n",
    "os.environ['PATH'] = f'{os.path.dirname(FFMPEG)}:' + os.environ['PATH']\n",
    "!which ffmpeg\n",
    "!which ffprobe\n",
    "\n",
    "root = git.Repo('.', search_parent_directories=True).working_tree_dir\n",
    "work_dir = os.path.join(root, 'modules', 'av_hubert', 'avhubert')\n",
    "os.chdir(work_dir)\n",
    "print(f\"Changed working directory to {work_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dlib, cv2, os\n",
    "import numpy as np\n",
    "import skvideo\n",
    "import skvideo.io\n",
    "from tqdm import tqdm\n",
    "from preparation.align_mouth import landmarks_interpolate, crop_patch, write_video_ffmpeg\n",
    "from IPython.display import HTML\n",
    "from base64 import b64encode\n",
    "\n",
    "def play_video(video_path, width=200):\n",
    "    mp4 = open(video_path,'rb').read()\n",
    "    data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
    "    return HTML(f\"\"\"\n",
    "    <video width={width} controls>\n",
    "        <source src=\"{data_url}\" type=\"video/mp4\">\n",
    "    </video>\n",
    "    \"\"\")\n",
    "\n",
    "def detect_landmark(image, detector, predictor):\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "    rects = detector(gray, 1)\n",
    "    coords = None\n",
    "    for (_, rect) in enumerate(rects):\n",
    "        shape = predictor(gray, rect)\n",
    "        coords = np.zeros((68, 2), dtype=np.int32)\n",
    "        for i in range(0, 68):\n",
    "            coords[i] = (shape.part(i).x, shape.part(i).y)\n",
    "    return coords\n",
    "\n",
    "def preprocess_video(input_video_path, output_video_path, face_predictor_path, mean_face_path):\n",
    "    detector = dlib.get_frontal_face_detector()\n",
    "    predictor = dlib.shape_predictor(face_predictor_path)\n",
    "    STD_SIZE = (256, 256)\n",
    "    mean_face_landmarks = np.load(mean_face_path)\n",
    "    stablePntsIDs = [33, 36, 39, 42, 45]\n",
    "    videogen = skvideo.io.vread(input_video_path)\n",
    "    frames = np.array([frame for frame in videogen])\n",
    "    landmarks = []\n",
    "    for frame in tqdm(frames):\n",
    "        landmark = detect_landmark(frame, detector, predictor)\n",
    "        landmarks.append(landmark)\n",
    "    preprocessed_landmarks = landmarks_interpolate(landmarks)\n",
    "    rois = crop_patch(input_video_path, preprocessed_landmarks, mean_face_landmarks, stablePntsIDs, STD_SIZE, \n",
    "                        window_margin=12, start_idx=48, stop_idx=68, crop_height=96, crop_width=96)\n",
    "    write_video_ffmpeg(rois, output_video_path, FFMPEG)\n",
    "    return\n",
    "\n",
    "play_video(f'/mmfs1/gscratch/intelligentsystems/bandhav/av-proc/data/misc/avhubert_demo_video_8s.mp4', width=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import tempfile\n",
    "import torch\n",
    "import utils as avhubert_utils\n",
    "from argparse import Namespace\n",
    "import fairseq\n",
    "from fairseq import checkpoint_utils, options, tasks, utils\n",
    "from IPython.display import HTML\n",
    "\n",
    "def extract_visual_feature(video_path, ckpt_path, user_dir):\n",
    "    utils.import_user_module(Namespace(user_dir=user_dir))\n",
    "    models, saved_cfg, task = checkpoint_utils.load_model_ensemble_and_task([ckpt_path])\n",
    "    transform = avhubert_utils.Compose([\n",
    "        avhubert_utils.Normalize(0.0, 255.0),\n",
    "        avhubert_utils.CenterCrop((task.cfg.image_crop_size, task.cfg.image_crop_size)),\n",
    "        avhubert_utils.Normalize(task.cfg.image_mean, task.cfg.image_std)])\n",
    "    frames = avhubert_utils.load_video(video_path)\n",
    "    frames = transform(frames)\n",
    "    frames = torch.FloatTensor(frames).unsqueeze(dim=0).unsqueeze(dim=0).cuda()\n",
    "    model = models[0]\n",
    "    if hasattr(models[0], 'decoder'):\n",
    "        model = models[0].encoder.w2v_model\n",
    "    model.cuda()\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Specify output_layer if you want to extract feature of an intermediate layer\n",
    "        feature, _ = model.extract_finetune(\n",
    "            source={'video': frames, 'audio': None}, padding_mask=None, output_layer=None)\n",
    "        feature = feature.squeeze(dim=0)\n",
    "    return feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_predictor_path = f\"{root}/data/misc/shape_predictor_68_face_landmarks.dat\"\n",
    "mean_face_path = f\"{root}/data/misc/20words_mean_face.npy\"\n",
    "origin_clip_path = f\"{root}/data/misc/avhubert_demo_video_8s.mp4\"\n",
    "ckpt_path = f\"{root}/data/checkpoints/base_vox_433h.pt\"\n",
    "user_dir = f\"{root}/modules/av_hubert/avhubert\"\n",
    "\n",
    "dlib.DLIB_USE_CUDA = True\n",
    "\n",
    "def extract_visual_features_from_video(\n",
    "    origin_clip_path, face_predictor_path, mean_face_path, ckpt_path, user_dir\n",
    "):\n",
    "    # Create a temporary file for mouth_roi_path\n",
    "    with tempfile.NamedTemporaryFile(suffix='.mp4') as mouth_roi:\n",
    "        mouth_roi_path = mouth_roi.name\n",
    "        # Call the preprocess_video function\n",
    "        preprocess_video(origin_clip_path, mouth_roi_path, face_predictor_path, mean_face_path)\n",
    "\n",
    "        # Call the feature extraction function\n",
    "        feature = extract_visual_feature(mouth_roi_path, ckpt_path, user_dir)\n",
    "\n",
    "    # Return the feature variable\n",
    "    return feature\n",
    "\n",
    "features = extract_visual_features_from_video(\n",
    "    f'{root}/data/misc/avhubert_demo_video_8s.mp4', face_predictor_path, mean_face_path, ckpt_path, user_dir\n",
    ")\n",
    "features.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "avhubert",
   "language": "python",
   "name": "avhubert"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
